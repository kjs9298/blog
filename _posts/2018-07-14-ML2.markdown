---
layout: post
title: GDG 1주차
date: 2018-07-13 17:52:00 +0900
description: 머신러닝
img: machine-learning.png
categories: [machine-learning]
---
## 머신러닝을 학습하면 더 잘할 수 있는 것 
1. 프로그래밍 시간을 줄일 수 있는 도구를 얻을 수 있다.
2. 커스티마이징하여 특정 범위에 적합한 상품을 제공할 수 있다. (재사용을 할 수 있음)
3. 어떻게 수동으로 하는지를 모르는 문제들을 풀 수 있다. (사람 표정을 읽거나, 친구들하고 대화를 하는 것처럼)
   알고리즘이 무엇인지 말할 필요가 없고 단순히 예시들을 돌려보고 문제를 풀 수 있으면 된다.
   
## 머신러닝은?
* 입력값들을 가지고 새로운 일을 예측하는 것.

## Label, Feature, Model
```
- labeled example : (x, y)
- unlabeled example : (x, ?)
```
* 라벨이 있는 예시로 학습을 시키고, 라벨이 없는 예시로 예측한다.
* 특성과 라벨의 관계를 정의한 것을 모델이라고 하고,
* 이러한 모델을 만드거나 배우는 것이 학습이다.
* 예시들을 보고 모델이 관계를 점차 학습해가는 것이다.

-> 좋은 특성과 좋은 라벨은 객관적인 수량으로 표현할 수 있는 것이다.  

## ML로 전환하기
### y = wx + b
* w : weight vector ? 무엇? slope 선형 모델에서 특성의 계수 또는 심층 네트워크의 엣지입니다.
  * 심층 네트워크가 뭐여 
* b : bias
* 실제값 - 예측값 = 손실값
* 손실은 항상 0 이상의 양수임
* l2 loss (예측값 - 신제값)^2 왜 제곱을 하지?
  * 면적이네
  * MSE (Mean Squared Error)
  * 왜 RMSE가 더 해석하기 쉽슴?
* 그래서 실제값이 예측값에서 멀어질수록 손실은 제곱으로 커진다고 함.
* w, b는 우리가 정의하는 것인가?
* 이러한 그래프들은 주로 있는 레파토리를 분석한 것인가?
* 추론은 x를 조정하는 것이구만
* regression은 정확히 뭐지 연속값을 출력하는 모델

## 손실 줄이기
### 반복 방식
```
y = wx + b
```
* w랑 b를 어떻게 동시에 추정을 하지? 근거를 해서 다음 값을 넣는 것인가? 아니면 그냥 아무값이나 넣는 것인가?
* 그래서 이렇게 주루루루룩가다가 손실이 없어질때까지 수행하는 것임

### 경사하강법
* 오.. 위에 대한 답이 됨
* 무작정 하는 것은 당연히 비효율적임
* 모든 w에 대한 좌표는 항아리모양을 하고 있음. 그래서 우리의 목표인 손실이 작은 최하점으로 가는 것이 목표임
* 아래로 가기 위해서 우리는 편미분의 벡터를 구해서 기울기를 계산해야함
* 그러면 결론적으로 기울기가 0이 되는 지점을 찾는 것인가?

### 학습률
* 초매개변수?
  * 사용자 본인에 의해서 조작되는 값
  * 매개변수와 대비됨 (그럼 매개변수는 사용자에 의해서 하는 것이 아니라는 말임?)
  * 맞넹 매개변수는 ML 스스로가 정하는 값을 말함
* 골디락스 학습률
* 학습률은 초매개변수임
  * = 보폭

### 학습률 최적화
* 그래서 우리가 해야하는 일은 바람직한 학습률을 구하는 것임
* 당연히 학습단계가 제일 작아야, 최적화되었다고 말할 수 있겠지
* 학습률이 매우 크다고, 또는 매우 작다고 최적화될수있는 것은 아님
* 어느 순간이 있음
* 우리는 모든 경우에 최적된 학습률을 찾을 필요는 없음
* 경사하강법이 효과적으로 수렴할 정도로 크지만 발산하지 않을 정도로 작은 적당한 학습률을 구하면 됨

### 확률적 경사하강법
* 반복당 하나의 예만 사용하는 것
* 1개의 반복에서 사용하는 예의 총 개수를 배치라고 하는데,
* 당연히 배치가 크면 시간이 개 오래걸릴 것임. (물론 정확하긴 하겠지)
* 그래서 데이터가 어마어마하게 커지게 되면 다 계산하고 앉아있을 수가 없음
* 그것을 위해서 1개의 반복에서 1개의 예만 사용하는 것을 확률적 경사하강법이라고 함.
* 확률론적(Stochastic)이란 말은 랜덤하게 뽑은 다는 말과 같음 (딱히 확률적이진 않구만)
* 전체 배치와 확률적 경사하강법 중간 단계가 바로 미니 배치 확률적 경사하강법임
  * 얘는 노이즈는 줄이고
  * 전체 배치보다는 시간도 줄어드는 효과가 있음
* 근데 반복 자체가 다른 변수값으로 수행을 한다는 것인데 반복과 예의 차이점은 뭘까?

## TF첫걸음
### TPU?
* 하드웨어넹

* 오오.. 안에는 C++로 되어 있고 Python은 그놈을 래핑할 뿐이군
* 그래프 프로토콜 버퍼, 분산된 그래프를 실행하는 런타임 -> JVM 같더라 여러 하드웨어에서 돌아갈 수 있게 하는 것.

### Estimator
* 로직 자체를 캡슐화한 것

### 텐서
* 대부분 스칼라, 벡터 또는 행렬로 이루어진 N차원 데이터 구조
* 저게 뭔말임ㅋㅋㅋㅋㅋㅋㅋ 벡터란 말임 행렬이란 말임

### 판다스(Pandas)

### Tensorflow
* california_housing_dataframe[["total_rooms"]] 이중괄호랑 그냥 괄호랑 출력되는게 다름 

## 일반화
### 과적합의 위험
* 필요 이상으로 복잡한 모델을 만들면 과적합이 발생한다.
* 머신러닝의 근본적인 과제는 데이터 적합도를 유지하는 동시에 최대한 단순화하는 것이다.
* 왜 간단한 것이 더 좋은 결과를 얻을 수 있을까?
* 학습세트랑 테스트 세트랑 구분을 하네
  * 테스트 세트는 학습하는데 사용하지 않니?
* 학습데이터랑 비슷하면 새 데이터를 못맞춤 ㅋㄷ
* 여기서 차원은 무슨 말이징  


----

